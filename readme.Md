# E-Commerce Product Scraper, Analyzer, and Viewer

This project is a web application built with Django to scrape, structure, and analyze product data from an e-commerce website. It features:

1. Web scraping using BeautifulSoup or Scrapy to gather product details.
2. Data storage using Django's ORM.
3. Analysis and summarization using the Groq API.
4. Display of scraped data and insights in a user-friendly web interface.

## Features

1. **Web Scraping**
   - Scrapes up to 200 products from an e-commerce website.
   - Collects details like product name, price, rating, and description.

2. **Data Structuring**
   - Cleans and structures the scraped data.

3. **Database Storage**
   - Stores structured data in a relational database (SQLite by default).

4. **Data Analysis**
   - Uses the Groq API to generate insights and summaries of the scraped data.

5. **UI for Display**
   - Displays product details in a table.
   - Provides a dedicated section for Groq API-generated summaries.

## Technologies Used

- **Backend**: Django, Python
- **Scraping**: BeautifulSoup or Scrapy
- **Database**: SQLite (can be switched to MySQL/PostgreSQL)
- **Frontend**: HTML, CSS, TailwindCSS (optional)
- **API**: Groq API

## Setup Instructions

### Prerequisites

1. Python (>=3.10)
2. pip (Python package manager)
3. A Groq API key
4. A GitHub or GitLab account (for repository setup)

### Steps to Run the Project

1. **Clone the Repository**

   ```bash
   git clone <repository-url>
   cd <repository-directory>
   ```

2. **Set Up Virtual Environment**

   ```bash
   python -m venv env
   source env/bin/activate   # For Linux/Mac
   env\Scripts\activate     # For Windows
   ```

3. **Install Dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Set Up Environment Variables**

   Create a `.env` file in the project root and add the following:

   ```env
   SECRET_KEY=<your_django_secret_key>
   DEBUG=True
   DATABASE_URL=sqlite:///db.sqlite3
   GROQ_API_KEY=<your_groq_api_key>
   ```

5. **Run Database Migrations**

   ```bash
   python manage.py migrate
   ```

6. **Run the Scraper**

   ```bash
   python manage.py runscript scraper
   ```

   This will scrape product data and store it in the database.

7. **Start the Server**

   ```bash
   python manage.py runserver
   ```

8. **Access the Application**

   Open your browser and navigate to [http://127.0.0.1:8000](http://127.0.0.1:8000).

## Project Structure

```plaintext
project_root/
├── ecommerce/
│   ├── settings.py  # Django project settings
│   ├── urls.py      # Project URLs
│   ├── wsgi.py      # WSGI entry point
├── products/
│   ├── migrations/  # Database migrations
│   ├── models.py    # Django models
│   ├── views.py     # Views for the application
│   ├── urls.py      # App-specific URLs
│   ├── scraper.py   # Web scraping logic
│   ├── groq_summary.py  # Integration with Groq API
├── templates/
│   ├── base.html    # Base template
│   ├── products/    # Product templates
├── db.sqlite3       # Default database file
├── requirements.txt # Python dependencies
├── manage.py        # Django management script
├── .env             # Environment variables
├── README.md        # Project documentation
```

## Usage

1. **Product List**: Navigate to `/products/` to view the table of products.
2. **Insights**: Scroll to the insights section to see summaries generated by the Groq API.

## Contribution

Feel free to fork the repository, create a branch, and submit a pull request for any enhancements or fixes.

## License

This project is open-source and available under the [MIT License](LICENSE).
